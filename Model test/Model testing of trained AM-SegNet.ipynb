{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing of trained AM-SegNet:\n",
    "\n",
    "# Construct model structure\n",
    "# Load trained model weights\n",
    "# Load images for model testing\n",
    "# Peform semantic segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for model development\n",
    "from keras import models, layers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of lightweight block (lw_conv_block)\n",
    "\n",
    "def lw_conv_block(inputs, filter_size, filter_num, dropout, batch_norm=True):\n",
    "\n",
    "    conv = layers.Conv2D(filter_num, (1, 1), padding='same')(inputs)\n",
    "    conv = layers.Activation('relu')(conv)\n",
    "\n",
    "    #left\n",
    "    squeeze_conv = layers.Conv2D(filter_num,(1, 1), padding='same')(conv)\n",
    "\n",
    "    # Batch normalization operation\n",
    "    if batch_norm is True:\n",
    "        squeeze_conv = layers.BatchNormalization(axis=3)(squeeze_conv)\n",
    "\n",
    "    squeeze_conv = layers.Activation(\"relu\")(squeeze_conv)\n",
    "\n",
    "    #middle\n",
    "    channel_conv = layers.DepthwiseConv2D(kernel_size=(filter_size, filter_size), strides=(1, 1), padding='same', depth_multiplier=2)(conv)\n",
    "    point_conv = layers.Conv2D(2*filter_num, (1, 1), padding='same')(channel_conv)\n",
    "    \n",
    "    # Batch normalization operation\n",
    "    if batch_norm is True:\n",
    "        separa_conv = layers.BatchNormalization(axis=3)(point_conv)\n",
    "    else:\n",
    "        separa_conv = point_conv\n",
    "        \n",
    "    separa_conv = layers.Activation(\"relu\")(separa_conv)\n",
    "\n",
    "    #right\n",
    "    expand_conv = layers.Conv2D(filter_num,(filter_size, filter_size), padding='same')(conv)\n",
    "    \n",
    "    # Batch normalization operation\n",
    "    if batch_norm is True:\n",
    "        expand_conv = layers.BatchNormalization(axis=3)(expand_conv)\n",
    "\n",
    "    expand_conv = layers.Activation(\"relu\")(expand_conv)\n",
    "\n",
    "    #concatenate\n",
    "    lw_conv = layers.concatenate([squeeze_conv, separa_conv, expand_conv], axis=3)\n",
    "    \n",
    "    # Dropout operation\n",
    "    if dropout > 0:\n",
    "        lw_conv = layers.Dropout(dropout)(lw_conv)\n",
    "\n",
    "    return lw_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of convolution block\n",
    "\n",
    "def stand_conv_block(inputs, filter_size, filter_num, dropout, batch_norm=True):\n",
    "\n",
    "    conv = layers.Conv2D(filter_num, (filter_size, filter_size), padding='same')(inputs)\n",
    "    \n",
    "    # Batch normalization operation\n",
    "    if batch_norm is True:\n",
    "\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "\n",
    "    conv_output = layers.Activation('relu')(conv)\n",
    "\n",
    "    # Dropout operation\n",
    "    if dropout > 0:\n",
    "        \n",
    "        conv_output = layers.Dropout(dropout)(conv_output)\n",
    "\n",
    "    return conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of attention block\n",
    "\n",
    "def attention_block(x, gating, size):\n",
    "\n",
    "# Convert gating single using (1,1) convolutions\n",
    "    phi_g = layers.Conv2D(size, (1, 1), padding='same')(gating) \n",
    "\n",
    "# Convert x single to the same shape as the gating signal\n",
    "    theta_x = layers.Conv2D(size, (1, 1), padding='same')(x) \n",
    "\n",
    "# Adding phi_g, theta_x together, activated by relu\n",
    "    concat_xg = layers.add([phi_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "\n",
    "# Conduct Ïˆ operation on act_xg, then activated by sigmoid\n",
    "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "\n",
    "# Conduct multiply operation on [upsample_psi, x]\n",
    "    result = layers.multiply([sigmoid_xg, x])\n",
    "    result_bn = layers.BatchNormalization()(result)\n",
    "    \n",
    "    return result_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of repeat_elem for element repeating\n",
    "\n",
    "def repeat_elem(tensor, rep):\n",
    "     \n",
    "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of AM-SegNet with lightweight block and attention mechanism\n",
    "\n",
    "def AM_SegNet(input_shape, num_classes, dropout, batch_norm):\n",
    "\n",
    "    # parameters of network congfiguration\n",
    "\n",
    "    filter_num = 12 # number of filters\n",
    "    filter_size = 3 # size of filters\n",
    "    up_samp_size = 2 # size of upsampling filters\n",
    "\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "\n",
    "    # Downsampling step 1\n",
    "    conv_1 = lw_conv_block(inputs, filter_size, 1*filter_num, dropout, batch_norm)\n",
    "    pool_1 = layers.MaxPooling2D(pool_size=(2,2))(conv_1)\n",
    "\n",
    "    # Downsampling step 2\n",
    "    conv_2 = lw_conv_block(pool_1, filter_size, 2*filter_num, dropout, batch_norm)\n",
    "    pool_2 = layers.MaxPooling2D(pool_size=(2,2))(conv_2)\n",
    "\n",
    "    # Downsampling step 3\n",
    "    conv_3 = lw_conv_block(pool_2, filter_size, 4*filter_num, dropout, batch_norm)\n",
    "    pool_3 = layers.MaxPooling2D(pool_size=(2,2))(conv_3)\n",
    "\n",
    "    # Downsampling step 5\n",
    "    conv_4 = lw_conv_block(pool_3, filter_size, 8*filter_num, dropout, batch_norm)\n",
    "    pool_4 = layers.MaxPooling2D(pool_size=(2,2))(conv_4)\n",
    "\n",
    "    # Standard convolution only\n",
    "    conv_5_1 = stand_conv_block(pool_4, filter_size, 64*filter_num, dropout, batch_norm)\n",
    "    conv_5_2 = stand_conv_block(conv_5_1, filter_size, 64*filter_num, dropout, batch_norm)\n",
    "    conv_5_3 = stand_conv_block(conv_5_2, filter_size, 64*filter_num, dropout, batch_norm)\n",
    "\n",
    "    #Calculate Attention\n",
    "    conv_att = attention_block(conv_5_1, conv_5_3, 64*filter_num)\n",
    "    conv_5 = layers.add([conv_5_3, conv_att])\n",
    "\n",
    "    # Upsampling\n",
    "\n",
    "    # Upsampling step 1\n",
    "\n",
    "    up_1 = layers.UpSampling2D(size=(up_samp_size, up_samp_size), data_format=\"channels_last\")(conv_5)\n",
    "    up_1 = layers.concatenate([up_1, conv_4], axis=3)\n",
    "    up_conv_1 = stand_conv_block(up_1, filter_size, 16*filter_num, dropout, batch_norm)\n",
    "   \n",
    "    # Upsampling step 2\n",
    "    up_conv_1 = layers.Conv2D(16*filter_num, (filter_size, filter_size), padding='same')(up_conv_1)\n",
    "    up_2 = layers.UpSampling2D(size=(up_samp_size, up_samp_size), data_format=\"channels_last\")(up_conv_1)\n",
    "    up_2 = layers.concatenate([up_2, conv_3], axis=3)\n",
    "    up_conv_2 = stand_conv_block(up_2, filter_size, 8*filter_num, dropout, batch_norm)\n",
    "\n",
    "    # Upsampling step 3\n",
    "    up_conv_2 = layers.Conv2D(8*filter_num, (filter_size, filter_size), padding='same')(up_conv_2)\n",
    "    up_3 = layers.UpSampling2D(size=(up_samp_size, up_samp_size), data_format=\"channels_last\")(up_conv_2)\n",
    "    up_3 = layers.concatenate([up_3, conv_2], axis=3)\n",
    "    up_conv_3 = stand_conv_block(up_3, filter_size, 4*filter_num, dropout, batch_norm)\n",
    "\n",
    "    # Upsampling step 4\n",
    "    up_conv_3 = layers.Conv2D(4*filter_num, (filter_size, filter_size), padding='same')(up_conv_3)\n",
    "    up_4 = layers.UpSampling2D(size=(up_samp_size, up_samp_size), data_format=\"channels_last\")(up_conv_3)\n",
    "    up_4 = layers.concatenate([up_4, conv_1], axis=3)\n",
    "    up_conv_4 = stand_conv_block(up_4, filter_size, 2*filter_num, dropout, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "    conv_final = layers.Conv2D(num_classes, kernel_size=(1,1))(up_conv_4)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n",
    "    \n",
    "    # Model \n",
    "    model = models.Model(inputs, conv_final, name=\"AM-SegNet\")\n",
    "    \n",
    "    # print model summary for details\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of images and input shape\n",
    "input_size_x= 256 \n",
    "input_size_y= 512\n",
    "input_size = (input_size_x,input_size_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of pixel labels: Keyhole, pore, substract, background and powder\n",
    "class_num=5\n",
    "\n",
    "#Setting dropout rate\n",
    "dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the AM-SegNet model\n",
    "model = AM_SegNet(input_size, class_num, dropout, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load X-ray images for model testing\n",
    "test_images = []\n",
    "\n",
    "for directory_path in glob.glob(\"test images/\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        img = cv2.imread(img_path, 0)       \n",
    "        test_images.append(img)\n",
    "       \n",
    "#Convert image list to np.array       \n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained model weights\n",
    "model.load_weights('Model weights of AM-SegNet.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform semantic segmentation analysis\n",
    "y_pred=model.predict(test_images)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot segmention results using customised colormap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Customised color map\n",
    "colors = ['#541352FF', '#d62728', '#2f9aa0FF', '#10a53dFF', '#ffcf20FF']\n",
    "custom_cmap = ListedColormap(colors)\n",
    "\n",
    "#Plot the segmention results, e.g. the first one in the image list\n",
    "figure(figsize=(8, 4))\n",
    "plt.imshow(y_pred_argmax[0], cmap= custom_cmap, alpha=0.60)\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1c0582f30543fff29a5261a1abd13119b0f120c9034b64de73a59460f47c2f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
